# -*- coding: utf-8 -*-
"""Untitled21.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_xofvTw3xsLR5kskKjibsdYouXA__VpN
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader
import torchvision
import torchvision.transforms as transforms
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm
import os

# ============================================================================
# MASKED CONVOLUTION LAYERS (for PixelCNN)
# ============================================================================

class MaskedConv2d(nn.Conv2d):
    """
    Masked convolution for PixelCNN.
    Mask Type A: excludes the center pixel (used in first layer)
    Mask Type B: includes the center pixel (used in subsequent layers)
    """
    def __init__(self, mask_type, *args, **kwargs):
        super(MaskedConv2d, self).__init__(*args, **kwargs)
        assert mask_type in ['A', 'B'], "mask_type must be 'A' or 'B'"
        self.register_buffer('mask', self.weight.data.clone())

        _, _, kH, kW = self.weight.size()
        self.mask.fill_(1)

        # Zero out the appropriate regions
        # For a kH x kW kernel, center is at (kH//2, kW//2)
        center_h = kH // 2
        center_w = kW // 2

        # Zero out everything below the center row
        self.mask[:, :, center_h + 1:, :] = 0

        # Zero out everything to the right of center in the center row
        self.mask[:, :, center_h, center_w + 1:] = 0

        # For mask type A, also zero out the center pixel
        if mask_type == 'A':
            self.mask[:, :, center_h, center_w] = 0

    def forward(self, x):
        self.weight.data *= self.mask
        return super(MaskedConv2d, self).forward(x)

# ============================================================================
# PIXELCNN MODEL
# ============================================================================

class PixelCNN(nn.Module):
    """
    PixelCNN model using masked convolutions.
    Architecture follows the paper's description with residual connections.
    """
    def __init__(self, n_channels=3, n_filters=64, n_layers=7, n_classes=256):
        super(PixelCNN, self).__init__()

        self.n_classes = n_classes

        # First layer: 7x7 masked conv type A
        self.conv1 = MaskedConv2d('A', n_channels, n_filters,
                                   kernel_size=7, padding=3, bias=False)
        self.bn1 = nn.BatchNorm2d(n_filters)

        # Residual blocks with mask type B
        self.res_blocks = nn.ModuleList()
        for _ in range(n_layers):
            self.res_blocks.append(
                ResidualBlock(n_filters, n_filters)
            )

        # Output layers - predict distribution over pixel values
        # Separate predictions for each color channel
        self.out_conv = nn.Sequential(
            nn.Conv2d(n_filters, n_filters, 1),
            nn.ReLU(True),
            nn.Conv2d(n_filters, n_channels * n_classes, 1)
        )

    def forward(self, x):
        # First masked conv
        out = F.relu(self.bn1(self.conv1(x)))

        # Residual blocks
        for block in self.res_blocks:
            out = block(out)

        # Output layer
        out = self.out_conv(out)

        # Reshape to (batch, channels, classes, height, width)
        batch_size, _, height, width = x.shape
        out = out.view(batch_size, self.n_classes, -1, height, width)

        return out

class ResidualBlock(nn.Module):
    """Residual block with masked convolutions (type B)"""
    def __init__(self, n_filters, n_out_filters):
        super(ResidualBlock, self).__init__()
        self.conv1 = MaskedConv2d('B', n_filters, n_out_filters // 2,
                                   kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm2d(n_out_filters // 2)

        self.conv2 = MaskedConv2d('B', n_out_filters // 2, n_out_filters // 2,
                                   kernel_size=3, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(n_out_filters // 2)

        self.conv3 = MaskedConv2d('B', n_out_filters // 2, n_out_filters,
                                   kernel_size=1, bias=False)
        self.bn3 = nn.BatchNorm2d(n_out_filters)

    def forward(self, x):
        residual = x
        out = F.relu(self.bn1(self.conv1(x)))
        out = F.relu(self.bn2(self.conv2(out)))
        out = self.bn3(self.conv3(out))
        return F.relu(out + residual)

# ============================================================================
# ROW LSTM MODEL
# ============================================================================

class RowLSTM(nn.Module):
    """
    Row LSTM model as described in PixelRNN paper.
    Uses convolutions for input-to-state and state-to-state transitions.
    """
    def __init__(self, n_channels=3, n_filters=64, n_lstm_layers=7, n_classes=256):
        super(RowLSTM, self).__init__()

        self.n_channels = n_channels
        self.n_filters = n_filters
        self.n_classes = n_classes

        # Initial convolution
        self.input_conv = nn.Conv2d(n_channels, n_filters,
                                     kernel_size=3, padding=1)

        # Row LSTM layers
        self.lstm_layers = nn.ModuleList()
        for i in range(n_lstm_layers):
            self.lstm_layers.append(
                RowLSTMLayer(n_filters, n_filters)
            )

        # Output layer
        self.output_conv = nn.Sequential(
            nn.Conv2d(n_filters, n_filters, 1),
            nn.ReLU(True),
            nn.Conv2d(n_filters, n_channels * n_classes, 1)
        )

    def forward(self, x):
        batch_size, _, height, width = x.shape

        # Initial convolution
        h = self.input_conv(x)

        # Process through Row LSTM layers
        for lstm in self.lstm_layers:
            h = lstm(h)

        # Output
        out = self.output_conv(h)
        out = out.view(batch_size, self.n_classes, self.n_channels, height, width)

        return out

class RowLSTMLayer(nn.Module):
    """
    Single Row LSTM layer.
    Processes image row by row from top to bottom.
    """
    def __init__(self, input_dim, hidden_dim):
        super(RowLSTMLayer, self).__init__()

        self.input_dim = input_dim
        self.hidden_dim = hidden_dim

        # Input-to-state convolution (1xk convolution)
        self.i2s = nn.Conv2d(input_dim, 4 * hidden_dim,
                            kernel_size=(1, 3), padding=(0, 1))

        # State-to-state convolution (1x1 convolution)
        self.s2s = nn.Conv2d(hidden_dim, 4 * hidden_dim,
                            kernel_size=1, padding=0)

    def forward(self, x):
        batch_size, _, height, width = x.shape

        # Initialize hidden and cell states
        h = torch.zeros(batch_size, self.hidden_dim, 1, width,
                       device=x.device)
        c = torch.zeros(batch_size, self.hidden_dim, 1, width,
                       device=x.device)

        outputs = []

        # Process row by row
        for i in range(height):
            x_row = x[:, :, i:i+1, :]

            # Compute gates
            i2s_out = self.i2s(x_row)
            s2s_out = self.s2s(h)
            gates = i2s_out + s2s_out

            # Split into gates: input, forget, output, cell
            i_gate, f_gate, o_gate, g_gate = torch.chunk(gates, 4, dim=1)

            # Apply activations
            i_gate = torch.sigmoid(i_gate)
            f_gate = torch.sigmoid(f_gate)
            o_gate = torch.sigmoid(o_gate)
            g_gate = torch.tanh(g_gate)

            # Update cell and hidden states
            c = f_gate * c + i_gate * g_gate
            h = o_gate * torch.tanh(c)

            outputs.append(h)

        # Concatenate all rows
        output = torch.cat(outputs, dim=2)

        return output

# ============================================================================
# DIAGONAL BiLSTM MODEL
# ============================================================================

class DiagonalBiLSTM(nn.Module):
    """
    Diagonal BiLSTM model as described in PixelRNN paper.
    Uses skewing to process diagonals with bidirectional LSTMs.
    """
    def __init__(self, n_channels=3, n_filters=64, n_lstm_layers=7, n_classes=256):
        super(DiagonalBiLSTM, self).__init__()

        self.n_channels = n_channels
        self.n_filters = n_filters
        self.n_classes = n_classes

        # Initial convolution
        self.input_conv = nn.Conv2d(n_channels, n_filters,
                                     kernel_size=3, padding=1)

        # Diagonal BiLSTM layers
        self.lstm_layers = nn.ModuleList()
        for i in range(n_lstm_layers):
            self.lstm_layers.append(
                DiagonalBiLSTMLayer(n_filters, n_filters)
            )

        # Output layer
        self.output_conv = nn.Sequential(
            nn.Conv2d(n_filters, n_filters, 1),
            nn.ReLU(True),
            nn.Conv2d(n_filters, n_channels * n_classes, 1)
        )

    def forward(self, x):
        batch_size, _, height, width = x.shape

        # Initial convolution
        h = self.input_conv(x)

        # Process through Diagonal BiLSTM layers
        for lstm in self.lstm_layers:
            h = lstm(h)

        # Output
        out = self.output_conv(h)
        out = out.view(batch_size, self.n_classes, self.n_channels, height, width)

        return out

class DiagonalBiLSTMLayer(nn.Module):
    """
    Single Diagonal BiLSTM layer.
    Skews input, applies bidirectional LSTM along columns, then unskews.
    """
    def __init__(self, input_dim, hidden_dim):
        super(DiagonalBiLSTMLayer, self).__init__()

        self.input_dim = input_dim
        self.hidden_dim = hidden_dim

        # Forward and backward LSTMs
        self.lstm_forward = nn.LSTM(input_dim, hidden_dim // 2, batch_first=True)
        self.lstm_backward = nn.LSTM(input_dim, hidden_dim // 2, batch_first=True)

    def skew(self, x):
        """Skew the input map for diagonal processing"""
        batch_size, channels, height, width = x.shape

        # Pad on the right
        x_padded = F.pad(x, (0, height - 1, 0, 0))

        # Skew by shifting each row
        skewed = []
        for i in range(height):
            row = x_padded[:, :, i, i:i + width]
            skewed.append(row)

        skewed = torch.stack(skewed, dim=2)
        return skewed

    def unskew(self, x, original_height, original_width):
        """Unskew the output back to original shape"""
        batch_size, channels, height, width = x.shape

        # Reverse the skewing operation
        unskewed = []
        for i in range(height):
            if i + original_width <= width:
                row = x[:, :, i, i:i + original_width]
            else:
                # Handle edge case
                row = F.pad(x[:, :, i, i:], (0, original_width - (width - i)))
            unskewed.append(row)

        unskewed = torch.stack(unskewed, dim=2)
        return unskewed[:, :, :original_height, :original_width]

    def forward(self, x):
        batch_size, channels, height, width = x.shape

        # Skew the input
        x_skewed = self.skew(x)

        # Reshape for LSTM: (batch * width, height, channels)
        x_reshaped = x_skewed.permute(0, 3, 2, 1).contiguous()
        x_reshaped = x_reshaped.view(batch_size * width, height, channels)

        # Apply forward LSTM
        out_fwd, _ = self.lstm_forward(x_reshaped)

        # Apply backward LSTM (reverse sequence)
        x_reversed = torch.flip(x_reshaped, dims=[1])
        out_bwd, _ = self.lstm_backward(x_reversed)
        out_bwd = torch.flip(out_bwd, dims=[1])

        # Concatenate forward and backward
        out = torch.cat([out_fwd, out_bwd], dim=2)

        # Reshape back
        out = out.view(batch_size, width, height, self.hidden_dim)
        out = out.permute(0, 3, 2, 1).contiguous()

        # Unskew
        out = self.unskew(out, height, width)

        return out

# ============================================================================
# TRAINING AND EVALUATION
# ============================================================================

def discretize_images(images, n_classes=256):
    """Convert images from [0, 1] to discrete values in [0, n_classes-1]"""
    return (images * (n_classes - 1)).long()

def calculate_bits_per_dim(loss, n_pixels, n_classes=256):
    """Convert negative log-likelihood to bits per dimension"""
    return loss / (np.log(2) * n_pixels)

def train_epoch(model, train_loader, optimizer, device, n_classes=256):
    """Train for one epoch"""
    model.train()
    total_loss = 0
    total_pixels = 0

    pbar = tqdm(train_loader, desc='Training')
    for images, _ in pbar:
        images = images.to(device)
        batch_size, channels, height, width = images.shape

        # Discretize images
        targets = discretize_images(images, n_classes)

        # Forward pass
        optimizer.zero_grad()
        outputs = model(images)

        # Reshape for cross entropy
        # outputs: (batch, n_classes, channels, height, width)
        # targets: (batch, channels, height, width)
        outputs = outputs.permute(0, 2, 1, 3, 4)  # (batch, channels, n_classes, h, w)
        outputs = outputs.reshape(-1, n_classes)
        targets = targets.view(-1)

        # Compute loss
        loss = F.cross_entropy(outputs, targets)

        # Backward pass
        loss.backward()
        optimizer.step()

        # Track metrics
        n_pixels = batch_size * channels * height * width
        total_loss += loss.item() * n_pixels
        total_pixels += n_pixels

        # Update progress bar
        avg_loss = total_loss / total_pixels
        bits_per_dim = calculate_bits_per_dim(avg_loss, 1, n_classes)
        pbar.set_postfix({'loss': f'{avg_loss:.4f}',
                         'bits/dim': f'{bits_per_dim:.4f}'})

    return total_loss / total_pixels

def evaluate(model, val_loader, device, n_classes=256):
    """Evaluate model on validation set"""
    model.eval()
    total_loss = 0
    total_pixels = 0

    with torch.no_grad():
        for images, _ in tqdm(val_loader, desc='Evaluating'):
            images = images.to(device)
            batch_size, channels, height, width = images.shape

            # Discretize images
            targets = discretize_images(images, n_classes)

            # Forward pass
            outputs = model(images)

            # Reshape for cross entropy
            outputs = outputs.permute(0, 2, 1, 3, 4)
            outputs = outputs.reshape(-1, n_classes)
            targets = targets.view(-1)

            # Compute loss
            loss = F.cross_entropy(outputs, targets)

            # Track metrics
            n_pixels = batch_size * channels * height * width
            total_loss += loss.item() * n_pixels
            total_pixels += n_pixels

    return total_loss / total_pixels

def train_model(model, train_loader, val_loader, device,
                n_epochs=50, lr=0.001, model_name='model'):
    """Complete training loop"""
    optimizer = optim.Adam(model.parameters(), lr=lr)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=5
    )

    train_losses = []
    val_losses = []
    best_val_loss = float('inf')

    for epoch in range(n_epochs):
        print(f'\nEpoch {epoch+1}/{n_epochs}')

        # Train
        train_loss = train_epoch(model, train_loader, optimizer, device)
        train_losses.append(train_loss)

        # Evaluate
        val_loss = evaluate(model, val_loader, device)
        val_losses.append(val_loss)

        # Calculate bits per dimension
        train_bpd = calculate_bits_per_dim(train_loss, 1)
        val_bpd = calculate_bits_per_dim(val_loss, 1)

        print(f'Train Loss: {train_loss:.4f} ({train_bpd:.4f} bits/dim)')
        print(f'Val Loss: {val_loss:.4f} ({val_bpd:.4f} bits/dim)')

        # Learning rate scheduling
        scheduler.step(val_loss)

        # Save best model
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save(model.state_dict(), f'{model_name}_best.pth')
            print(f'Saved best model with val loss: {val_loss:.4f}')

    return train_losses, val_losses

# ============================================================================
# MAIN EXECUTION
# ============================================================================

def main():
    # Hyperparameters
    batch_size = 64
    n_epochs = 5
    lr = 0.001
    n_filters = 64
    n_layers = 7
    n_classes = 256

    # Device
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f'Using device: {device}')

    # Determine number of workers based on system
    num_workers = 2 if torch.cuda.is_available() else 0

    # Load CIFAR-10 dataset
    transform = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_dataset = torchvision.datasets.CIFAR10(
        root='./data', train=True, download=True, transform=transform
    )
    val_dataset = torchvision.datasets.CIFAR10(
        root='./data', train=False, download=True, transform=transform
    )

    train_loader = DataLoader(train_dataset, batch_size=batch_size,
                            shuffle=True, num_workers=num_workers)
    val_loader = DataLoader(val_dataset, batch_size=batch_size,
                          shuffle=False, num_workers=num_workers)

    # Dictionary to store results
    results = {}

    # # Train PixelCNN
    print('\n' + '='*60)
    print('Training PixelCNN')
    print('='*60)
    pixelcnn = PixelCNN(n_channels=3, n_filters=n_filters,
                       n_layers=n_layers, n_classes=n_classes).to(device)
    train_losses_cnn, val_losses_cnn = train_model(
        pixelcnn, train_loader, val_loader, device, n_epochs, lr, 'pixelcnn'
    )
    results['PixelCNN'] = (train_losses_cnn, val_losses_cnn)

    # # Train Row LSTM
    print('\n' + '='*60)
    print('Training Row LSTM')
    print('='*60)
    row_lstm = RowLSTM(n_channels=3, n_filters=n_filters,
                      n_lstm_layers=n_layers, n_classes=n_classes).to(device)
    train_losses_row, val_losses_row = train_model(
        row_lstm, train_loader, val_loader, device, n_epochs, lr, 'row_lstm'
    )
    results['Row LSTM'] = (train_losses_row, val_losses_row)

    # Train Diagonal BiLSTM
    print('\n' + '='*60)
    print('Training Diagonal BiLSTM')
    print('='*60)
    diag_bilstm = DiagonalBiLSTM(n_channels=3, n_filters=n_filters,
                                 n_lstm_layers=n_layers, n_classes=n_classes).to(device)
    train_losses_diag, val_losses_diag = train_model(
        diag_bilstm, train_loader, val_loader, device, n_epochs, lr, 'diagonal_bilstm'
    )
    results['Diagonal BiLSTM'] = (train_losses_diag, val_losses_diag)

    # Plot results
    plot_results(results)

    print('\nTraining completed!')

def plot_results(results):
    """Plot training curves for all models"""
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    for model_name, (train_losses, val_losses) in results.items():
        # Convert to bits per dimension
        train_bpd = [calculate_bits_per_dim(loss, 1) for loss in train_losses]
        val_bpd = [calculate_bits_per_dim(loss, 1) for loss in val_losses]

        epochs = range(1, len(train_losses) + 1)

        axes[0].plot(epochs, train_bpd, label=model_name, linewidth=2)
        axes[1].plot(epochs, val_bpd, label=model_name, linewidth=2)

    axes[0].set_xlabel('Epoch')
    axes[0].set_ylabel('Bits per Dimension')
    axes[0].set_title('Training Performance')
    axes[0].legend()
    axes[0].grid(True)

    axes[1].set_xlabel('Epoch')
    axes[1].set_ylabel('Bits per Dimension')
    axes[1].set_title('Validation Performance')
    axes[1].legend()
    axes[1].grid(True)

    plt.tight_layout()
    plt.savefig('pixelrnn_comparison.png', dpi=300)
    plt.show()

if __name__ == '__main__':
    main()