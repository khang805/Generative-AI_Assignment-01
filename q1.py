# -*- coding: utf-8 -*-
"""Untitled21.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_xofvTw3xsLR5kskKjibsdYouXA__VpN
"""

# ======================
# 1. Install & Import Required Libraries
# ======================
import tensorflow as tf
from datasets import load_dataset
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, classification_report, precision_recall_fscore_support
import seaborn as sns
from tensorflow.keras import layers, models
import pandas as pd

# ======================
# 2. Load CIFAR-10 Dataset from Hugging Face
# ======================
print("Loading CIFAR-10 dataset...")
dataset = load_dataset("cifar10")

# Extract train and test datasets
train_dataset = dataset['train']
test_dataset = dataset['test']

print(f"Training samples: {len(train_dataset)}")
print(f"Test samples: {len(test_dataset)}")
print(f"Image shape: {train_dataset[0]['img'].size}")
print(f"Number of classes: 10")

# CIFAR-10 class names
class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',
               'dog', 'frog', 'horse', 'ship', 'truck']

# ======================
# 3. Dataset Preparation and Preprocessing
# ======================
def preprocess_data(dataset):
    """Convert dataset to numpy arrays and preprocess images"""
    images = []
    labels = []
    for item in dataset:
        # Convert PIL image to numpy array and normalize to [0, 1]
        img_array = np.array(item['img']) / 255.0
        images.append(img_array)
        labels.append(item['label'])
    return np.array(images), np.array(labels)

# Preprocess training and test data
print("Preprocessing training data...")
x_train, y_train = preprocess_data(train_dataset)
print("Preprocessing test data...")
x_test, y_test = preprocess_data(test_dataset)

print(f"Training data shape: {x_train.shape}")
print(f"Training labels shape: {y_train.shape}")
print(f"Test data shape: {x_test.shape}")
print(f"Test labels shape: {y_test.shape}")

# Split training data for validation (90% train, 10% validation)
split_index = int(0.9 * len(x_train))
x_val = x_train[split_index:]
y_val = y_train[split_index:]
x_train = x_train[:split_index]
y_train = y_train[:split_index]

print(f"After split - Train: {x_train.shape}, Validation: {x_val.shape}")

# ======================
# 4. Build CNN Model
# ======================
# ======================
# ======================
# 4. Build CNN Model (7 Conv Layers)
# ======================

def create_cnn_model():
    inputs = layers.Input(shape=(32, 32, 3))

    # Conv Block 1
    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)
    x = layers.BatchNormalization()(x)
    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.MaxPooling2D((2, 2))(x)   # only pool once per block
    x = layers.Dropout(0.25)(x)

    # Conv Block 2
    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.MaxPooling2D((2, 2))(x)
    x = layers.Dropout(0.25)(x)

    # Conv Block 3
    x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x)  # 7th Conv
    x = layers.BatchNormalization()(x)
    x = layers.MaxPooling2D((2, 2))(x)
    x = layers.Dropout(0.25)(x)

    # Dense layers
    x = layers.Flatten()(x)
    x = layers.Dense(512, activation='relu')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(0.5)(x)
    x = layers.Dense(256, activation='relu')(x)
    x = layers.Dropout(0.5)(x)
    outputs = layers.Dense(10, activation='softmax')(x)

    return models.Model(inputs, outputs)


# Create and compile the model
model = create_cnn_model()

# ======================
#  MANUAL LEARNING RATE
# ======================
initial_lr = 0.001   # ðŸ‘ˆ set custom learning rate here
optimizer = tf.keras.optimizers.Adam(learning_rate=initial_lr)

model.compile(optimizer=optimizer,
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Display model architecture
model.summary()

# ======================
# 5. Train the Model
# ======================
print("Starting model training...")

# Callbacks for better training
callbacks = [
    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),
    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)
]

# Train the model
history = model.fit(
    x_train, y_train,
    batch_size=64,
    epochs=30,
    validation_data=(x_val, y_val),
    callbacks=callbacks,
    verbose=1
)

# ======================
# 6. Plot Training History
# ======================
plt.figure(figsize=(15, 5))

# Plot training & validation loss values
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)

# Plot training & validation accuracy values
plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

# ======================
# 7. Evaluate Model Performance
# ======================
print("Evaluating model on test set...")
test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=0)
print(f"Test Loss: {test_loss:.4f}")
print(f"Test Accuracy: {test_accuracy:.4f}")

# Make predictions
y_pred = model.predict(x_test)
y_pred_classes = np.argmax(y_pred, axis=1)

# ======================
# 8. Confusion Matrix
# ======================
print("Generating confusion matrix...")
cm = confusion_matrix(y_test, y_pred_classes)

plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=class_names, yticklabels=class_names)
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.xticks(rotation=45)
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

# ======================
# 9. Performance Metrics Table
# ======================
print("Calculating performance metrics...")
precision, recall, f1, support = precision_recall_fscore_support(y_test, y_pred_classes, average=None)

# Create metrics table
metrics_table = pd.DataFrame({
    'Class': class_names,
    'Precision': precision,
    'Recall': recall,
    'F1-Score': f1,
    'Support': support
})

# Add overall metrics
overall_metrics = pd.DataFrame({
    'Class': ['OVERALL'],
    'Precision': [precision.mean()],
    'Recall': [recall.mean()],
    'F1-Score': [f1.mean()],
    'Support': [len(y_test)]
})
metrics_table = pd.concat([metrics_table, overall_metrics], ignore_index=True)

print("\n" + "="*60)
print("PERFORMANCE METRICS")
print("="*60)
print(metrics_table.round(4))

# ======================
# 10. Extract and Visualize Feature Maps
# ======================
def visualize_feature_maps(model, sample_image, layer_names):
    """Visualize feature maps from different convolutional layers"""
    layer_outputs = [model.get_layer(name).output for name in layer_names]
    feature_map_model = tf.keras.Model(inputs=model.input, outputs=layer_outputs)

    sample_image = np.expand_dims(sample_image, axis=0)

    feature_maps = feature_map_model.predict(sample_image)

    fig, axes = plt.subplots(len(layer_names), 4, figsize=(15, 4*len(layer_names)))
    for i, (layer_name, feature_map) in enumerate(zip(layer_names, feature_maps)):
        num_features = feature_map.shape[-1]
        for j in range(4):
            if len(layer_names) > 1:
                ax = axes[i, j]
            else:
                ax = axes[j]
            ax.imshow(feature_map[0, :, :, j], cmap='viridis')
            ax.set_title(f'{layer_name}\nFeature {j+1}')
            ax.axis('off')
    plt.tight_layout()
    plt.show()
    return feature_maps

# Select a sample image for visualization
sample_idx = 0
sample_image = x_test[sample_idx]
sample_label = y_test[sample_idx]
# print(f"Sample image: {class_names[sample_label]}")

# Auto-detect Conv2D layers
conv_layers_to_visualize = [layer.name for layer in model.layers if isinstance(layer, tf.keras.layers.Conv2D)]
print("Conv layers found:", conv_layers_to_visualize)

print("Visualizing feature maps...")
feature_maps = visualize_feature_maps(model, sample_image, conv_layers_to_visualize)

# ======================
# 11. Discussion of Convolutional Layers
# ======================
print("\n" + "="*60)
print("DISCUSSION: WHAT CONVOLUTIONAL LAYERS ARE DOING")
print("="*60)
print(f"""
 1. First Convolutional Layer ({conv_layers_to_visualize[0]}):
    - Detects basic features like edges, colors, and simple textures
    - Low-level feature extraction

 2. Middle Convolutional Layer ({conv_layers_to_visualize[1]}):
    - Combines basic features to detect more complex patterns
    - May identify shapes, corners, and texture combinations

 3. Deeper Convolutional Layer ({conv_layers_to_visualize[2]}):
    - Detects high-level features and object parts
    - Learns complex patterns specific to CIFAR-10 classes
""")

# ======================
# 13. Final Summary
# ======================
print("\n" + "="*60)
print("FINAL SUMMARY")
print("="*60)
print(f"Model Architecture: CNN with {len(model.layers)} layers")
print(f"Training Samples: {len(x_train)}")
print(f"Validation Samples: {len(x_val)}")
print(f"Test Samples: {len(x_test)}")
print(f"Final Test Accuracy: {test_accuracy:.4f}")
print(f"Final Test Loss: {test_loss:.4f}")
print(f"Number of Parameters: {model.count_params():,}")

# Calculate class-wise accuracy
class_accuracy = []
for i in range(10):
    class_mask = y_test == i
    class_correct = (y_pred_classes[class_mask] == y_test[class_mask]).sum()
    class_total = class_mask.sum()
    class_accuracy.append(class_correct / class_total)

plt.figure(figsize=(10, 6))
plt.bar(class_names, class_accuracy)
plt.title('Class-wise Accuracy')
plt.xlabel('Class')
plt.ylabel('Accuracy')
plt.xticks(rotation=45)
plt.ylim(0, 1)
plt.grid(axis='y', alpha=0.3)
plt.tight_layout()
plt.show()